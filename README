## ðŸ“‚ Project Structure
- `cuda/`: CUDA extension build + Python wrapper (`rec_llm_kernels`).
  - `cuda/csrc/`: C++/CUDA source code.
  - `cuda/python/rec_llm_kernels/`: Python package wrapper (imports `_C.so` after build).
- `runtime/`: Minimal inference runtime scaffolding (`rec_llm_runtime`).
- `neuron/`: Neuron/Inf2 backend package (`rec_llm_kernels_neuron`).
- `docs/`: Roadmap and Inf2 notes.

## Build & Install (CUDA)
This project contains CUDA (`.cu`) sources and generally needs **Linux + NVIDIA GPU + CUDA toolkit** to build.

### Create and activate venv
```bash
python3 -m venv venv
source venv/bin/activate
```

### Install core dependencies
```bash
pip install torch setuptools wheel ninja
```

### This will trigger the initial CUDA compilation
```bash
pip install -e cuda
```

### Recompiling C++/CUDA Kernels
If you modify any files in `cuda/csrc/` (`.cu` or `.cpp`), you must recompile the binary:

Option A: Fast in-place build (recommended for debugging).  
This generates the .so binary directly in your project folder for immediate testing.

```bash
python cuda/setup.py build_ext --inplace
```

Option B: Clean build (recommended for production).
```bash
MAX_JOBS=4 pip install -e cuda
```

### FlashInfer (optional)
By default, CMake will **not** fetch FlashInfer over the network. Choose one:

- Use a local checkout:
```bash
USE_FLASHINFER=1 pip install -e cuda --config-settings=cmake.args="-DFLASHINFER_SOURCE_DIR=/path/to/flashinfer"
```

- Allow CMake to fetch it (requires network access):
```bash
USE_FLASHINFER=1 pip install -e cuda --config-settings=cmake.args="-DFLASHINFER_FETCH=ON"
```

## Testing (pytest)
Kernel tests require CUDA. On non-CUDA machines, tests will be skipped.

```bash
pip install pytest
pytest -q tests_cuda
```

To show skip reasons in the output:
```bash
pytest -q -rs tests_cuda
```

To show a short test summary including skipped/failed/xfailed reasons:
```bash
pytest -ra tests_cuda
```

### Troubleshooting
- `CUDA error: invalid configuration argument` in `reshape_and_cache`: older kernel launch used a 2D thread block
  (`head_dim` x `num_heads`) which can exceed 1024 threads/block (e.g. `32 * 128 = 4096`). Rebuild after pulling
  the fix and the kernel will launch as one block per `(token, head)`.

### Colab rebuild (if extension import fails)
If you hit an error like `undefined symbol ... type_caster<at::Tensor>::load(...)` when importing `rec_llm_kernels._C`,
do a clean rebuild:

```bash
cd /content/rec_llm_kernels
rm -rf build
rm -rf *.egg-info
rm -rf **/__pycache__

pip uninstall -y rec-llm-kernels rec_llm_kernels || true
MAX_JOBS=2 pip install -e cuda -v
pytest -q tests_cuda
```

### Colab quickstart
In Colab, set Runtime â†’ Change runtime type â†’ GPU, then run:
```bash
git clone <YOUR_REPO_URL> rec_llm_kernels
cd rec_llm_kernels
pip -q install --upgrade pip
pip -q install torch pytest ninja setuptools wheel
pip -q install -e cuda
pytest -q tests_cuda
```

### Tiny Llama smoke test (Transformers)
Quick sanity check that a tiny Llama model can run on GPU and returns `past_key_values`:
```bash
pip -q install transformers accelerate sentencepiece
python scripts/colab_tiny_llama_smoke.py
```

## Kubernetes (model serving + training simulation)
This repo also includes a minimal Kubernetes-based model serving + training simulation system:

- **Inference Server Cluster**: 2 replicas behind a Service (load balanced)
  - Prompt processing: `POST /generate`
  - Model version tracking: response header `X-Model-Version`
  - Zero-downtime model updates: hot reload in-place (no pod restart) via mounted `ConfigMap` + `POST /reload`
- **Fake Trainer Server**:
  - Sends prompt requests to the inference Service
  - Emits model "weight" updates by patching the `ConfigMap` and triggering reload on all inference pods

See `/k8s/README.md` for minikube instructions and a quick demo.

## AWS Inf2 (Neuron)
See `/docs/INF2.md` for running the Neuron backend tests on Inf2.
